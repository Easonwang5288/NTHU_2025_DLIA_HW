{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "609dcb62-c2f8-4c6d-9c89-63dc0148a87c"
   },
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "###### Lab 2\n",
    "\n",
    "# National Tsing Hua University\n",
    "\n",
    "#### Spring 2025\n",
    "\n",
    "#### 11320IEEM 513600\n",
    "\n",
    "#### Deep Learning and Industrial Applications\n",
    "    \n",
    "## Lab 2: Predicting Heart Disease with Deep Learning\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "061c22d2-eec4-40f4-866b-ccaa2d9a2963",
    "tags": []
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In the realm of healthcare, early detection and accurate prediction of diseases play a crucial role in patient care and management. Heart disease remains one of the leading causes of mortality worldwide, making the development of effective diagnostic tools essential. This lab leverages deep learning to predict the presence of heart disease in patients using a subset of 14 key attributes from the Cleveland Heart Disease Database. The objective is to explore and apply deep learning techniques to distinguish between the presence and absence of heart disease based on clinical parameters.\n",
    "\n",
    "Throughout this lab, you'll engage with the following key activities:\n",
    "- Use [Pandas](https://pandas.pydata.org) to process the CSV files.\n",
    "- Use [PyTorch](https://pytorch.org) to build an Artificial Neural Network (ANN) to fit the dataset.\n",
    "- Evaluate the performance of the trained model to understand its accuracy.\n",
    "\n",
    "### Attribute Information\n",
    "\n",
    "1. age: Age of the patient in years\n",
    "2. sex: (Male/Female)\n",
    "3. cp: Chest pain type (4 types: low, medium, high, and severe)\n",
    "4. trestbps: Resting blood pressure\n",
    "5. chol: Serum cholesterol in mg/dl\n",
    "6. fbs: Fasting blood sugar > 120 mg/dl\n",
    "7. restecg: Resting electrocardiographic results (values 0,1,2)\n",
    "8. thalach: Maximum heart rate achieved\n",
    "9. exang: Exercise induced angina\n",
    "10. oldpeak: Oldpeak = ST depression induced by exercise relative to rest\n",
    "11. slope: The slope of the peak exercise ST segment\n",
    "12. ca: Number of major vessels (0-3) colored by fluoroscopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversible defect\n",
    "14. target: target have disease or not (1=yes, 0=no)\n",
    "\n",
    "### References\n",
    "- [UCI Heart Disease Data](https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data) for the dataset we use in this lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "ad594fc8-4989-40f3-b124-4550fe7df386"
   },
   "source": [
    "## A. Checking and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "42a3eafd-cbcd-4c56-82cb-83a0bfa2399e",
    "outputId": "ed5307f8-b55a-4d7f-e113-45f96b6d1591"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('heart_dataset_train_all.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34241797-60f0-4818-a44b-f5379948d621",
    "outputId": "a55c9a63-dc43-411b-ef46-2fb48db11690"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "026585db-a6d8-4062-85de-e3a7eaebed72",
    "outputId": "ed978caf-af66-4266-c951-17c64a0ed866"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69031e6d-0fb5-49d9-b723-a0d1fee08c3c",
    "outputId": "76dca713-ae8d-47bd-bbcb-491e8f249750"
   },
   "outputs": [],
   "source": [
    "# checking for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "cb3090f8-2cfa-4f56-8aa5-cf954bb19932"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38aadbee-d68f-4ae0-b842-b40800b0cac9",
    "outputId": "34aae2b5-7805-4cb1-81db-99a6a0cbd620"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "26a69fd5-3534-4d8e-b59a-6778bf47a479"
   },
   "outputs": [],
   "source": [
    "# Mapping 'sex' descriptions to numbers\n",
    "sex_description = {\n",
    "    'Male': 0,\n",
    "    'Female': 1,\n",
    "}\n",
    "df.loc[:, 'sex'] = df['sex'].map(sex_description)\n",
    "\n",
    "# Mapping 'cp' (chest pain) descriptions to numbers\n",
    "pain_description = {\n",
    "    'low': 0,\n",
    "    'medium': 1,\n",
    "    'high': 2,\n",
    "    'severe': 3\n",
    "}\n",
    "df.loc[:, 'cp'] = df['cp'].map(pain_description)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "051108c6-7011-4187-9e36-bd2944a019ca",
    "outputId": "ef50c90f-dc11-4bc8-d9e0-be42fd263593"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "8b999df5-09a1-4ce2-b068-f1afba448ff8",
    "outputId": "b687919c-7870-4c62-a58e-779e8db02f90"
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "8ce7a0c5-76d6-4863-ba61-0544a220962a"
   },
   "source": [
    "#### Converting the DataFrame to a NumPy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5735baad-2db8-4306-aa4c-7788d2b49621",
    "outputId": "db1c5b35-b477-452b-a96f-dbf4693471b5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_data = df.values\n",
    "np_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "29b8e189-7f39-435a-8038-39098b147325"
   },
   "outputs": [],
   "source": [
    "split_point = int(np_data.shape[0]*0.7)\n",
    "\n",
    "np.random.shuffle(np_data)\n",
    "\n",
    "x_train = np_data[:split_point, :13]\n",
    "y_train = np_data[:split_point, 13]\n",
    "x_val = np_data[split_point:, :13]\n",
    "y_val = np_data[split_point:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fedb56d7-1665-4c90-9697-b86cab43f300",
    "outputId": "2c7e2a91-55b4-472c-b3dd-2b799db31064"
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "#from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "#x_train = torch.from_numpy(x_train).float()\n",
    "#y_train = torch.from_numpy(y_train).long()\n",
    "#x_train = np.array(x_train, dtype=float)\n",
    "#y_train = np.array(y_train, dtype=int)\n",
    "#x_train = torch.from_numpy(x_train).float()\n",
    "#y_train = torch.from_numpy(y_train).long()\n",
    "\n",
    "\n",
    "#x_val = torch.from_numpy(x_val).float()\n",
    "#y_val = torch.from_numpy(y_val).long()\n",
    "\n",
    "#batch_size = 32\n",
    "\n",
    "# Create datasets\n",
    "#train_dataset = TensorDataset(x_train, y_train)\n",
    "#val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "# Create dataloaders\n",
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#print(f'Number of samples in train and validation are {len(train_loader.dataset)} and {len(val_loader.dataset)}.')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# 轉換成 NumPy 並清理缺失值或非數值資料\n",
    "x_train = np.nan_to_num(np.array(x_train, dtype=np.float32), nan=0.0)  # 清理 NaN 並強制轉換成 float32\n",
    "y_train = np.nan_to_num(np.array(y_train, dtype=np.int64), nan=0)      # 強制轉換成 int64，清理 NaN\n",
    "\n",
    "x_val = np.nan_to_num(np.array(x_val, dtype=np.float32), nan=0.0)      # 同樣處理驗證資料\n",
    "y_val = np.nan_to_num(np.array(y_val, dtype=np.int64), nan=0)\n",
    "\n",
    "# 轉換成 PyTorch tensor\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n",
    "x_val = torch.from_numpy(x_val).float()\n",
    "y_val = torch.from_numpy(y_val).long()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 建立 TensorDataset 與 DataLoader\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'Number of samples in train and validation are {len(train_loader.dataset)} and {len(val_loader.dataset)}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "8ffc26b9-6044-41e9-93e2-7dc6250dbd27"
   },
   "source": [
    "## B. Defining Neural Networks\n",
    "\n",
    "In PyTorch, we can use **class** to define our custom neural network architectures by subclassing the `nn.Module` class. This gives our neural network all the functionality it needs to work with PyTorch's other utilities and keeps our implementation organized.\n",
    "\n",
    "- Neural networks are defined by subclassing `nn.Module`.\n",
    "- The layers of the neural network are initialized in the `__init__` method.\n",
    "- The forward pass operations on input data are defined in the `forward` method.\n",
    "\n",
    "It's worth noting that while we only define the forward pass, PyTorch will automatically derive the backward pass for us, which is used during training to update the model's weights.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "77975746-a7a7-4676-9527-57674cd98c0f"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(13, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "cbb8b5b0-0ec0-406c-a42e-048aa00e05aa"
   },
   "source": [
    "## C. Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3602ae7d-4034-4c49-b221-0c12a5824b18",
    "outputId": "0f037155-638a-4e04-b59e-938616f2f39c"
   },
   "outputs": [],
   "source": [
    "# Check your GPU status.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "833c2a309ddf4479b911ef4be5217432",
      "4c699d87949340fb92106cd67305deba",
      "0af10bdd24c545b8879ef0df6bee12e2",
      "54a1b435d01f4c7798e02f728362d980",
      "a0856f0084c24a61ab01cac3d1226d94",
      "02f5b945c44949baa45893408e656101",
      "382db34be861422d81185d6065707e5f",
      "001f18667f9540a3af50fe00c7b50a2e",
      "72eab2184ecd479391708a045aa5396f",
      "0493cd52f15a470cbc507dcc8cff394f",
      "c3ab5427a1744374bd4f2ef580943c92"
     ]
    },
    "id": "f73a5c35-c15d-49bb-8a33-a7f017159499",
    "outputId": "7b8106f7-3bda-41c0-fd02-c1df5fdd3cca"
   },
   "outputs": [],
   "source": [
    "#import torch.optim as optim\n",
    "#from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "#from tqdm.auto import tqdm\n",
    "\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# train_accuracies = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# epochs = 100\n",
    "\n",
    "# model = Model()\n",
    "# # print(model)\n",
    "\n",
    "# best_val_loss = float('inf')\n",
    "# best_val_acc = -1\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     # Training\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     train_correct = 0\n",
    "#     total_train_samples = 0\n",
    "\n",
    "#     for features, labels in train_loader:\n",
    "#         features = features.cuda()\n",
    "#         labels = labels.cuda()\n",
    "\n",
    "#         outputs = model(features)\n",
    "\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_predicted = outputs.argmax(-1)\n",
    "#         train_correct += (train_predicted == labels).sum().item()\n",
    "#         total_train_samples += labels.size(0)\n",
    "\n",
    "#     # Learning rate update\n",
    "#     lr_scheduler.step()\n",
    "\n",
    "#     avg_train_loss = total_loss / len(train_loader)\n",
    "#     train_accuracy = 100. * train_correct / total_train_samples\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     total_val_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for features, labels in val_loader:\n",
    "#             features = features.cuda()\n",
    "#             labels = labels.cuda()\n",
    "\n",
    "#             outputs = model(features)\n",
    "\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             total_val_loss += loss.item()\n",
    "\n",
    "#             predicted = outputs.argmax(-1)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#             total += labels.size(0)\n",
    "\n",
    "#     avg_val_loss = total_val_loss / len(val_loader)\n",
    "#     val_accuracy = 100. * correct / total\n",
    "\n",
    "#     # Checkpoint\n",
    "#     if avg_val_loss < best_val_loss:\n",
    "#         best_val_loss = avg_val_loss\n",
    "\n",
    "#     if val_accuracy > best_val_acc:\n",
    "#         best_val_acc = val_accuracy\n",
    "#         torch.save(model.state_dict(), 'model_classification.pth')\n",
    "\n",
    "#     print(f'Epoch {epoch+1}/{epochs}, Train loss: {avg_train_loss:.4f}, Train acc: {train_accuracy:.4f}%, Val loss: {avg_val_loss:.4f}, Val acc: {val_accuracy:.4f}%, Best Val loss: {best_val_loss:.4f} Best Val acc: {best_val_acc:.2f}%')\n",
    "\n",
    "#     # Store performance\n",
    "#     train_losses.append(avg_train_loss)\n",
    "#     train_accuracies.append(train_accuracy)\n",
    "#     val_losses.append(avg_val_loss)\n",
    "#     val_accuracies.append(val_accuracy)\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# train_accuracies = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# epochs = 100\n",
    "\n",
    "# # 動態偵測是否有可用的 GPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = Model().to(device)  # 確保模型放到正確裝置\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# best_val_loss = float('inf')\n",
    "# best_val_acc = -1\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     # Training\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     train_correct = 0\n",
    "#     total_train_samples = 0\n",
    "\n",
    "#     for features, labels in train_loader:\n",
    "#         features, labels = features.to(device), labels.to(device)  # 動態指定裝置\n",
    "\n",
    "#         outputs = model(features)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_predicted = outputs.argmax(-1)\n",
    "#         train_correct += (train_predicted == labels).sum().item()\n",
    "#         total_train_samples += labels.size(0)\n",
    "\n",
    "#     # 更新學習率\n",
    "#     lr_scheduler.step()\n",
    "\n",
    "#     avg_train_loss = total_loss / len(train_loader)\n",
    "#     train_accuracy = 100.0 * train_correct / total_train_samples\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     total_val_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for features, labels in val_loader:\n",
    "#             features, labels = features.to(device), labels.to(device)  # 動態指定裝置\n",
    "\n",
    "#             outputs = model(features)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             total_val_loss += loss.item()\n",
    "\n",
    "#             predicted = outputs.argmax(-1)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#             total += labels.size(0)\n",
    "\n",
    "#     avg_val_loss = total_val_loss / len(val_loader)\n",
    "#     val_accuracy = 100.0 * correct / total\n",
    "\n",
    "#     # 儲存最佳模型\n",
    "#     if avg_val_loss < best_val_loss:\n",
    "#         best_val_loss = avg_val_loss\n",
    "\n",
    "#     if val_accuracy > best_val_acc:\n",
    "#         best_val_acc = val_accuracy\n",
    "#         torch.save(model.state_dict(), 'model_classification.pth')\n",
    "\n",
    "#     print(f'Epoch {epoch + 1}/{epochs}, Train loss: {avg_train_loss:.4f}, Train acc: {train_accuracy:.2f}%, '\n",
    "#           f'Val loss: {avg_val_loss:.4f}, Val acc: {val_accuracy:.2f}%, Best Val loss: {best_val_loss:.4f}, '\n",
    "#           f'Best Val acc: {best_val_acc:.2f}%')\n",
    "\n",
    "#     # 儲存訓練過程\n",
    "#     train_losses.append(avg_train_loss)\n",
    "#     train_accuracies.append(train_accuracy)\n",
    "#     val_losses.append(avg_val_loss)\n",
    "#     val_accuracies.append(val_accuracy)\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# # 1. 動態偵測是否有可用的 GPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # 2. 修改 Model 類別，支援動態裝置分配\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, device):\n",
    "#         super().__init__()\n",
    "#         self.device = device\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(13, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 2)\n",
    "#         ).to(self.device)  # 確保模型在正確裝置\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# # 3. 初始化模型並移動到正確裝置\n",
    "# model = Model(device=device).to(device)\n",
    "\n",
    "# # 其他訓練參數\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# lr_scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0)\n",
    "\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# train_accuracies = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# epochs = 100\n",
    "# best_val_loss = float('inf')\n",
    "# best_val_acc = -1\n",
    "\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     # Training\n",
    "#     model.train()\n",
    "#     total_loss = 0.0\n",
    "#     train_correct = 0\n",
    "#     total_train_samples = 0\n",
    "\n",
    "#     for features, labels in train_loader:\n",
    "#         features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "#         outputs = model(features)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_predicted = outputs.argmax(-1)\n",
    "#         train_correct += (train_predicted == labels).sum().item()\n",
    "#         total_train_samples += labels.size(0)\n",
    "\n",
    "#     # 更新學習率\n",
    "#     lr_scheduler.step()\n",
    "\n",
    "#     avg_train_loss = total_loss / len(train_loader)\n",
    "#     train_accuracy = 100.0 * train_correct / total_train_samples\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     total_val_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for features, labels in val_loader:\n",
    "#             features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "#             outputs = model(features)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             total_val_loss += loss.item()\n",
    "\n",
    "#             predicted = outputs.argmax(-1)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#             total += labels.size(0)\n",
    "\n",
    "#     avg_val_loss = total_val_loss / len(val_loader)\n",
    "#     val_accuracy = 100.0 * correct / total\n",
    "\n",
    "#     # 儲存最佳模型\n",
    "#     if avg_val_loss < best_val_loss:\n",
    "#         best_val_loss = avg_val_loss\n",
    "\n",
    "#     if val_accuracy > best_val_acc:\n",
    "#         best_val_acc = val_accuracy\n",
    "#         torch.save(model.state_dict(), 'model_classification.pth')\n",
    "\n",
    "#     print(f'Epoch {epoch + 1}/{epochs}, Train loss: {avg_train_loss:.4f}, Train acc: {train_accuracy:.2f}%, '\n",
    "#           f'Val loss: {avg_val_loss:.4f}, Val acc: {val_accuracy:.2f}%, Best Val loss: {best_val_loss:.4f}, '\n",
    "#           f'Best Val acc: {best_val_acc:.2f}%')\n",
    "\n",
    "#     # 儲存訓練過程\n",
    "#     train_losses.append(avg_train_loss)\n",
    "#     train_accuracies.append(train_accuracy)\n",
    "#     val_losses.append(avg_val_loss)\n",
    "#     val_accuracies.append(val_accuracy)\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from tqdm.auto import tqdm\n",
    "# import torch.nn as nn\n",
    "# import torch\n",
    "# import itertools\n",
    "\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# train_accuracies = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# # 動態偵測是否有可用的 GPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # 超參數組合\n",
    "# learning_rates = [0.001, 0.01, 0.1]\n",
    "# hidden_units = [128, 256, 512]\n",
    "# epochs = 50\n",
    "\n",
    "# # 儲存結果\n",
    "# results = []\n",
    "\n",
    "# # 定義模型\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, hidden_size):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(13, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# # 迭代所有超參數組合\n",
    "# for lr, hu in itertools.product(learning_rates, hidden_units):\n",
    "#     print(f\"\\nTesting with Learning Rate = {lr}, Hidden Units = {hu}\")\n",
    "#     model = Model(hidden_size=hu).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "\n",
    "#     best_val_acc = -1\n",
    "#     for epoch in tqdm(range(epochs)):\n",
    "#         model.train()\n",
    "#         total_loss = 0.0\n",
    "#         train_correct = 0\n",
    "#         total_train_samples = 0\n",
    "\n",
    "#         for features, labels in train_loader:\n",
    "#             features, labels = features.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(features)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "#             train_correct += (outputs.argmax(-1) == labels).sum().item()\n",
    "#             total_train_samples += labels.size(0)\n",
    "\n",
    "#         lr_scheduler.step()\n",
    "\n",
    "#         avg_train_loss = total_loss / len(train_loader)\n",
    "#         train_accuracy = 100.0 * train_correct / total_train_samples\n",
    "\n",
    "#         model.eval()\n",
    "#         total_val_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         total_val_samples = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for features, labels in val_loader:\n",
    "#                 features, labels = features.to(device), labels.to(device)\n",
    "#                 outputs = model(features)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 total_val_loss += loss.item()\n",
    "#                 val_correct += (outputs.argmax(-1) == labels).sum().item()\n",
    "#                 total_val_samples += labels.size(0)\n",
    "\n",
    "#         avg_val_loss = total_val_loss / len(val_loader)\n",
    "#         val_accuracy = 100.0 * val_correct / total_val_samples\n",
    "\n",
    "#         if val_accuracy > best_val_acc:\n",
    "#             best_val_acc = val_accuracy\n",
    "\n",
    "#     print(f\"Final Validation Accuracy for LR {lr}, HU {hu}: {best_val_acc:.2f}%\")\n",
    "#     results.append({'learning_rate': lr, 'hidden_units': hu, 'best_val_accuracy': best_val_acc})\n",
    "\n",
    "# print(\"\\nExperiment Completed!\")\n",
    "# print(\"Results:\")\n",
    "# for res in results:\n",
    "#     print(res)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 載入資料 ===\n",
    "# 假設您已有 train_loader, val_loader, test_loader\n",
    "# 若未定義這部分請讓我知道，我可以幫您補上\n",
    "\n",
    "# 動態偵測是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 超參數組合\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "hidden_units = [128, 256, 512]\n",
    "epochs = 50\n",
    "\n",
    "# 儲存最終結果\n",
    "results = []\n",
    "\n",
    "# 定義模型\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(13, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# === 訓練過程 ===\n",
    "# 記錄最佳模型的結果（選擇最佳 val_accuracy 的超參數組合）\n",
    "best_model_state = None\n",
    "best_overall_val_acc = -1\n",
    "best_hyperparams = None\n",
    "\n",
    "# 專門記錄一組視覺化用的資料\n",
    "train_losses_plot = []\n",
    "val_losses_plot = []\n",
    "train_accuracies_plot = []\n",
    "val_accuracies_plot = []\n",
    "\n",
    "for lr, hu in itertools.product(learning_rates, hidden_units):\n",
    "    print(f\"\\nTraining with Learning Rate = {lr}, Hidden Units = {hu}\")\n",
    "    model = Model(hidden_size=hu).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
    "\n",
    "    best_val_acc = -1\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(-1) == labels).sum().item()\n",
    "            total_train_samples += labels.size(0)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = 100.0 * train_correct / total_train_samples\n",
    "\n",
    "        # 驗證模式\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total_val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                val_correct += (outputs.argmax(-1) == labels).sum().item()\n",
    "                total_val_samples += labels.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = 100.0 * val_correct / total_val_samples\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "\n",
    "    print(f\"Final Validation Accuracy for LR {lr}, HU {hu}: {best_val_acc:.2f}%\")\n",
    "    results.append({'learning_rate': lr, 'hidden_units': hu, 'best_val_accuracy': best_val_acc})\n",
    "\n",
    "    # 若為最佳模型則儲存\n",
    "    if best_val_acc > best_overall_val_acc:\n",
    "        best_overall_val_acc = best_val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "        best_hyperparams = {'learning_rate': lr, 'hidden_units': hu}\n",
    "        # 儲存這組資料供視覺化\n",
    "        train_losses_plot = train_losses.copy()\n",
    "        val_losses_plot = val_losses.copy()\n",
    "        train_accuracies_plot = train_accuracies.copy()\n",
    "        val_accuracies_plot = val_accuracies.copy()\n",
    "\n",
    "# 儲存最佳模型\n",
    "torch.save(best_model_state, 'model_classification.pth')\n",
    "print(f\"\\nBest Hyperparameters: {best_hyperparams}, Val Accuracy: {best_overall_val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "a7984c6e-6652-4160-b572-07d48bc93a3f"
   },
   "source": [
    "#### Visualizing the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "5559d850-1fb5-4b04-b6ca-60c5b309f34e",
    "outputId": "d4bb0b3e-fd06-48d6-d50a-7c0eecaea572"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "#  # Plotting training and validation accuracy\n",
    "# ax[0].plot(train_accuracies)\n",
    "# ax[0].plot(val_accuracies)\n",
    "# ax[0].set_title('Model Accuracy')\n",
    "# ax[0].set_xlabel('Epochs')\n",
    "# ax[0].set_ylabel('Accuracy')\n",
    "# ax[0].legend(['Train', 'Val'])\n",
    "\n",
    "#  # Plotting training and validation loss\n",
    "# ax[1].plot(train_losses)\n",
    "# ax[1].plot(val_losses)\n",
    "# ax[1].set_title('Model Loss')\n",
    "# ax[1].set_xlabel('Epochs')\n",
    "# ax[1].set_ylabel('Loss')\n",
    "# ax[1].legend(['Train', 'Val'])\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # 資料整理\n",
    "# learning_rates = [0.001, 0.001, 0.001, 0.01, 0.01, 0.01, 0.1, 0.1, 0.1]\n",
    "# hidden_units = [128, 256, 512] * 3\n",
    "# accuracies = [79.01, 79.01, 79.01, 79.01, 85.19, 81.48, 49.38, 50.62, 50.62]\n",
    "\n",
    "# # 視覺化\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for lr in sorted(set(learning_rates)):\n",
    "#     subset = [accuracies[i] for i in range(len(accuracies)) if learning_rates[i] == lr]\n",
    "#     plt.plot(hidden_units[:3], subset, marker='o', label=f'Learning Rate = {lr}')\n",
    "\n",
    "# plt.xlabel(\"Hidden Units\")\n",
    "# plt.ylabel(\"Validation Accuracy (%)\")\n",
    "# plt.title(\"Effect of Learning Rate and Hidden Units on Validation Accuracy\")\n",
    "# plt.xticks(hidden_units[:3])\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# === 視覺化 ===\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax[0].plot(train_accuracies_plot, label='Train Accuracy')\n",
    "ax[0].plot(val_accuracies_plot, label='Val Accuracy')\n",
    "ax[0].set_title('Model Accuracy')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy (%)')\n",
    "ax[0].legend()\n",
    "\n",
    "# Loss\n",
    "ax[1].plot(train_losses_plot, label='Train Loss')\n",
    "ax[1].plot(val_losses_plot, label='Val Loss')\n",
    "ax[1].set_title('Model Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "89c7e51b-8ab6-4aa2-877d-39b6daf55c20"
   },
   "source": [
    "## D. Evaluating Your Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "f49735d7-466f-4037-8078-172f03dffd8d",
    "outputId": "e815ef48-71b9-4f9d-98d8-61e3ca1ac734"
   },
   "outputs": [],
   "source": [
    "# read test file\n",
    "test_data = pd.read_csv('heart_dataset_test.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21ae9d85-0dc2-4db0-a7c7-807c6b6c514f",
    "outputId": "d43458a1-1a66-47f6-fcdb-6111da6be2d9"
   },
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ff2812b-a5a5-4ea9-86be-ae2143cb2ba7",
    "outputId": "d143e201-d151-4d90-cabc-9719fb87b48f"
   },
   "outputs": [],
   "source": [
    "test_data = test_data.values\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "14d4be20-f64f-421d-8971-e1e47873aef8"
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "x_test = torch.from_numpy(test_data[:, :13]).float()\n",
    "y_test = torch.from_numpy(test_data[:, 13]).long()\n",
    "\n",
    "# Create datasets\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bcf8580-42ee-4ee7-ad15-9f080cc57a33",
    "outputId": "8b2ea734-c445-46e2-b24b-c911b86e1ba0"
   },
   "outputs": [],
   "source": [
    "# # Load the trained weights\n",
    "# model.load_state_dict(torch.load('model_classification.pth'))\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# test_correct = 0\n",
    "# test_total = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for features, labels in test_loader:\n",
    "\n",
    "#         features = features.cuda()\n",
    "#         labels = labels.cuda()\n",
    "\n",
    "#         outputs = model(features)\n",
    "\n",
    "#         predicted = outputs.argmax(-1)\n",
    "#         test_correct += (predicted == labels).sum().item()\n",
    "#         test_total += labels.size(0)\n",
    "\n",
    "# print(f'Test accuracy is {100. * test_correct / test_total}%')\n",
    "\n",
    "# # 動態偵測是否有可用的 GPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Load the trained weights to the correct device\n",
    "# model.load_state_dict(torch.load('model_classification.pth', map_location=device))\n",
    "\n",
    "# # Set the model to evaluation mode and move it to the correct device\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# test_correct = 0\n",
    "# test_total = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for features, labels in test_loader:\n",
    "#         # 移動 features 和 labels 到對應的裝置\n",
    "#         features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "#         outputs = model(features)\n",
    "#         predicted = outputs.argmax(-1)\n",
    "\n",
    "#         test_correct += (predicted == labels).sum().item()\n",
    "#         test_total += labels.size(0)\n",
    "\n",
    "# print(f'Test accuracy is {100. * test_correct / test_total:.2f}%')\n",
    "\n",
    "# === 測試驗證 ===\n",
    "print(\"\\n--- Running on Test Set ---\")\n",
    "# 用最佳的 hidden_units 初始化模型\n",
    "model = Model(hidden_size=best_hyperparams['hidden_units']).to(device)\n",
    "model.load_state_dict(torch.load('model_classification.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        predicted = outputs.argmax(-1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f'Test Accuracy: {100.0 * test_correct / test_total:.2f}%')\n",
    "\n",
    "# === 顯示所有實驗結果 ===\n",
    "print(\"\\nAll Hyperparameter Results:\")\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
